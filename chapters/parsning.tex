\section{Parsning}

Implementeringen av ett programmeringsspråk finns i flera varianter och en
vanlig sådan är kompilatorn. Denna implementation läser input och producerar
sedan ett körbart program utgående från de instruktioner den fått.  Själva
processen av kompilering består av flera faser och komponenter. Den första
komponenten är en parser som läser input och konstruerar en maskinläslig
struktur utgående från den grammatik som givits. Vid detta skede bryr sig
programmet inte ännu om vad som skall göras utan den försöker enbart
identifiera de olika reglerna och granska att dess syntax är korrekt. Parsern
körs normalt som en komponent inne i en tolk vars funktion i sin tur är att
förstå och tolka innebördet hos en regel. När parsern är klar med sin analys
returnerar den strukturen till tolken som i sin tur returnerar sin modifierade
version av strukturen tillbaka till huvudkomponenten, kompilatorn. Kompilatorn
fungerar som en översättare som slutligen genererar den kod som datorns
processor kan förstå. En översikt av dessa komponenter och dess funktioner
visas i figur~\ref{fig:compiler} \citep[s. 16]{pt10}.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{figures/output/language-stages.pdf}
  \caption{Översikt av komponenterna i en kompilator.}
  \label{fig:compiler}
\end{figure}

Parsningsprocessen kan delas upp i två skilda faser, först en s.k.
lexikal analys som identifierar lexikala element, som kallas tokens. Tokens
är identifierbara teckensträngar med speciell betydelse. De kategoriseras
enligt typer såsom nyckelord, konstanter, parametrar osv. \citep[s. 6]{aa06}.

Den andra fasen som sker efter att tokens är identifierade är den syntaktiska
analysen där elementen sammansätts till helhetsuttryck.

\subsection{Lexikal analys}

Eftersom elementen i en lexikal analys kan beskrivas med en reguljär grammatik
använder man sig ofta av en ändlig automat för att läsa den. Denna typ av
automat brukar man kalla för lexer. Automaten börjar i ett specifikt start
läge var den väntar på ett tecken att läsa. När ett tecken läses går den genom
en serie alternativa lösningar den har för att minska mängden slutgiltiga
lösningar. När därpå följande tecken läses in fortsätter den att härleda sig
vidare tills den når en slutgiltig lösning, eller alternativt inte känner igen
elementet och skapar ett felmeddelande. När lösningen är hittad skickar lexern
elementet tillbaka till parsern och återgår till sitt utgångsläge för att
vänta på nästa tecken. På detta sätt kan lexern, som har en effektiv
algoritm, avlägsna onödig information såsom mellanslag och kommentarer för
att sedan ge uttryckets riktiga element vidare till parsern som nu enkelt vet
om en teckensträng är en nyckelordsterminal eller ett tal \citep[s.
51]{sm09}.

\subsection{Syntaktisk analys}

\begin{figure}[ht]
  \includegraphics[width=8cm]{figures/output/tree.pdf}
  \caption{Den syntaktiska uppdelningen av en kod-sats i Lua.}
\end{figure}

\subsection{Parsertyper}

Som det nämnts tidigare använder de flesta parsers en lexer för att
göra en lexikal analys av de tokens som ges som indata. När parsern får dessa
tokens påbörjar den en syntaktisk analys som kan implementeras på diverse
olika sätt. Huvudsakligen existerar två typer. Antingen härleds regler från
vänster och kallas då LL-parser eller så härleds regler från höger och kallas
LR-parser \citep[s. 67]{sm09}. En steg för steg jämförelse av de två
algoritmerna som använders för LL- och LR-parsers finns i
figur~\ref{fig:ll-vs-lr} på sida \pageref{fig:ll-vs-lr}.

När man beskriver parsers nämner man ofta hur långt fram den kan se innan den
gör ett beslut, dvs. hur många tokens framåt den kan se. Detta antal skriver
man inom en parentes efter parsertypen. För att uttrycka den vanligaste
formen av parser som är en LR-parser som ser en token framåt skriver man
LR(1) \citep[s. 69]{sm09}.

\subsubsection{LL-parser}

En LL-parser går att skriva för hand eftersom den i allmänhet följer en logisk
tankegång. De börjar med att gissa sig till en rot-regel, likt en ändlig
automat, och förväntar sig sedan att löv-reglerna skall passa in en efter en.
Om det misslyckas kan den gå till nästa alternativa regel och fortsätter
fram tills den bevisat en gissning. Denna typ av strategi kallas även för en
\textit{``uppifrån-och-ner''} algoritm \citep[s. 67]{sm09}.

\subsubsection{LR-parser}

LR-parsers genereras huvudsakligen av maskiner eftersom de är svårare att
visualisera. Dessa parsers börjar med att läsa löv-reglerna, alltså de minsta
identifierbara reglerna och ansluter dem sedan till varandra fram till det att
den nått en slutgiltig rot-regel. Detta kallas även för en
\textit{``nerifrån-och-upp''} algoritm \citep[s. 67]{sm09}.

Både LL-parsers och LR-parsers används i kompilatorer men LR är mer
vanligt \citep[s. 67]{sm09}. En orsak är att LR-parsers täcker en
större andel av grammatiker och därmed använder programvara som genererar
parsers ofta \textit{``nerifrån-och-upp''} algoritm \citep[s. 61]{aa06}.

\begin{figure}[ht]
  \begin{minipage}[t]{0.5\textwidth}
    % @TODO
    % \input{figures/tex/llparser}
  \end{minipage}%
  \begin{minipage}[t]{0.5\textwidth}
    % @TODO
    % \input{figures/tex/lrparser}
  \end{minipage}%
  \caption{Steg för steg parsning av ett matemtiskt uttryck med en
    ``uppifrån-och-ner''-algoritm (vänster) och en
    ``nerifrån-och-upp''-algoritm (höger).}
  \label{fig:ll-vs-lr}
\end{figure}

\subsubsection{Rekursivt nedstigande parser}

En form av LL-parsers som är vanlig för handskrivna parsers är den rekurisvt
nedstigande parsern. Istället för att utnyttja en automat fungerar den genom
att knyta varje icke-terminal i grammatik till en egen funktion som ansvarar
för att identifiera dess grammatiska löv. För varje icke-terminal i sin egen
regel kallar den rekursivt vidare på dess anknytande funktioner fram till det
att rot funktionen får hela syntaxträdet returnerat \citep[s. 24]{pt10}.

Flödet av en rekursivt nedstigande parser hittas i
figur~\ref{fig:recursivedescent} där varje nod symboliserar en funktion och
röda pilar symboliserar dess anrop medan blåa pilar symboliserar dess returnering.

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{figures/output/recursivedescent.pdf}
  \caption{Flöde av en rekursivt nedstigande parser steg för steg.}
  \label{fig:recursivedescent}
\end{figure}

\subsection{Tekniker}



\subsubsection{Vänsterfaktorering}

Ett vanligt problem grammatiker i en parser med \textit{``uppifrån-och-ner''}
algoritmen är vänsterrekursion. Detta innebär att produktionsregel använder
sig själv som icke-terminal längst till vänster i sammansättningen enligt. När
detta inträffar kommer implementationen fastna i en oändlig upprepning. För
att implementera en LL-parser måste man därför använda en metod som kallas
vänsterfaktorering.

Exempelvis är produktionsregeln för kalkylatorn i figur~\ref{fig:cfg} på sida
\pageref{fig:cfg} vänsterrekursiv.

\setlength{\grammarindent}{5em}
\begin{grammar}
  \singlespace\small%
  \fontfamily{lmr}\selectfont
  <uttryck> ::= <tal>
    \alt "(" <uttryck> ")"
    \alt <uttryck> <operator> <uttryck>
\end{grammar}

Genom att expandera rekursionen i alla existerande villkor som nya villkor får
vi:

\setlength{\grammarindent}{5em}
\begin{grammar}
  \singlespace\small%
  \fontfamily{lmr}\selectfont
  <uttryck> ::= <tal>
    \alt "(" <uttryck> ")"
    \alt <tal> <operator> <uttryck>
    \alt "(" <uttryck> ")" <operator> <uttryck>
\end{grammar}

Detta beskriver fortfarande en identisk grammatik och följande steg är att
förenkla uttrycket med en ny produktionsregel som utnyttjar $\epsilon$, en
symbol för att representera ett tomt värde.

\setlength{\grammarindent}{5em}
\begin{grammar}
  \singlespace\small%
  \fontfamily{lmr}\selectfont
  <uttryck> ::= <tal> <uttryck'>
    \alt "(" <uttryck> ")" <uttryck'>

  <uttryck'> ::= <tal>
    \alt "(" <uttryck> ")"
    \alt <operator> <uttryck>
    \alt $\epsilon$
\end{grammar}

Grammatikens funktion är fortfarande identisk men vänstrekursionen som är
omöjlig att utföra i en \textit{``uppifrån-och-ner''} algoritm är eliminerad.

En förenklad matematisk regel som kan användas för att vänsterfaktorera
$A \rightarrow A\alpha\;|\;\beta$ är \citep[s. 212]{aa06}:
\begin{align*}
A &\rightarrow \beta A^\prime \\
A^\prime &\rightarrow \alpha A^\prime\;|\;\epsilon
\end{align*}

\subsubsection{Backtracking}

I vissa fall är det användbart att inte ha en begränsad framåtblick och då kan
man använda sig av en teknik som heter backtracking som tillåter parsern att
se en obegränsad mängd tokens framåt. Implementationen av en backtracker
varierar men i enkel form fungerar den genom att när mångtydlighet uppstår
välja ett alternativ och sedan ha möjligheten att återgå till ett tidigare
tillstånd när alternativet bevisats vara korrekt eller felaktig \citep[s. 55]{pt10}.

För att uttrycka att en parser kan se en obegränas mängd tokens framåt anger
man antalet som k i formen LL(k) eller LR(k).

\subsubsection{Memoisation}

Kort om vad memoisation är.

\subsection{Parser-generatorer}

Att implementera en parser för hand är en tidskrävande och felbenägen process.
Produktionsregler påminner mycket om varandra och repetitionen att
implementera varje regel blir en långdragen uppgift med möjlighet för
introduktion av fel vid varje steg. På grund av detta har det blivit populärt
att använda en s.k. parser-generator som förser programmeraren med en färdig
parser implementation utgående från en given grammatik \citep[s. 26]{pt10}.

Med hjälp av en parser-generator kan implementatören upprätthålla grammatiken
av ett språk genom att skriva och modifiera en BNF-liknande syntax istället
för den egentliga koden. Detta har stora fördelar om grammatiken inte är helt
stabil eftersom enbart några rader behöver korrigeras vid förändringar. För
ett språk med en stabil grammatik existerar fördelarna endast vid
implementationsskedet och kan vara en nackdel i situationer som felsökning och
skapandet av läsbara felmeddelanden för syntaxfel \citep[s. 175]{bf09}.

Uppdelningen av genererade parsers och handskrivna parsers är delad, de flesta
språk använder sig av en genererad parser men några såsom C-språkets GCC \citep{gcc}
samt Luas egna parser \citep{luaimp} har börjat från en genererad parser och
sedan övergått till en handskriven rekursivt nedstigande parser för utökad
funktionalitet.

Parser-generatorer existerar för flera språk och för diverse grammatik- samt
algoritmtyper. I detta kapitel kommer jag att presentera två
parser-generatorer som är vanliga i unix-system.

\subsubsection{Lex}

Lex är en generator för reguljära grammatiker skapad av Mike Lesk och
Eric Schmidt. Verktyget använder en egen grammatik syntax för att
definiera produktionsregler i ett språk och generera sedan en lexer i C eller
Ratfor. För varje produktionsregel kan användaren definiera en åtgärd som
kommer att exekveras när en regel identifierats. Vanligen är detta C kod för
att returnera värdet till en parser så som Yacc eller Bison. Eftersom Lex
genererar kod existerar funktionalitet för att inkludera valfri C kod som
kopieras till resultat filen. Detta kan användas till att inkludera bibliotek
eller definiera funktioner som produktionsreglerna använder \citep{lex}.

I figur~\ref{fig:lex} hittas en enkel grammatik specifikation som läser input
och identifierar variabeldeklarationer av strängar eller heltal samt ignorerar
blanksteg.

\begin{figure}[ht]
  \lstinputlisting[title="",keywords={printf,return,include,main}]%
    {figures/tex/lex.l}
  \caption{En Lex-specifikation för att identifiera variabeldeklarationer av
    strängar eller heltal.}
  \label{fig:lex}
\end{figure}

\subsubsection{Yacc}

Eftersom de flesta programmeringsspråk använder sig av en kontextfri grammatik
är inte Lex verktyget tillräkligt utan man behöver ytterligare en syntaktisk
analysator. Yacc är en parser-generator för kontextfrigrammatik skapad av
Stephen Johnson. Syntaxen för grammatik specifikationen liknar mycket Lex.
Användaren definierar produktionsregler samt dess åtgärder men har nu
möjligheten att använda rekursion. Ytterligare bör man definiera en
lexikaliseringsfunktion, exempelvis \textit{yylex()}, samt vilka tokentyper
som existerar. Integreringen måste ske både i den valfria lexern och i
Yacc-specifikationen, t.ex. med en delad definitionsfil av tokens som lexern
returnerar och den Yacc-genererade parsern kan identifiera. Ett
pseudokod-exempel på ett språk som tillåter variabeldeklarationer av heltal
och strängar hittas i figur~\ref{fig:yacc}. För att parsern skall fungera
måste logiken för \textit{input} samt lexikaliseringen av \textit{VARIABLE},
\textit{STRING},
\textit{INTEGER} och \textit{EQUAL_SIGN} definieras i lexer koden.

\begin{figure}[ht]
  \lstinputlisting[title="",keywords={printf,return,include,main,do,while}]%
    {figures/tex/yacc.y}
  \caption{En Yacc-specifikation för variabeldeklarationer av heltal och
    strängar.}
  \label{fig:yacc}
\end{figure}

% vim: set tw=78:ts=2:sw=2:et:fdm=marker:wrap:wm=78:ft=tex
% vim: spell spelllang=sv
